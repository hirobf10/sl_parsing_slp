{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict, deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        dataset = [line.splitlines()[0] for line in f.readlines()]\n",
    "    return dataset\n",
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, bos_token=\"<s>\", eos_token=\"</s>\"):\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.prob_transition = None\n",
    "        self.prob_emission = None\n",
    "        self.pos = None\n",
    "        \n",
    "    def _get_count_dict(self, dataset: List[str], split_token=\"_\"):\n",
    "        cnt_transition = defaultdict(lambda: defaultdict(int))\n",
    "        cnt_emission = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for line in dataset:\n",
    "            token_pos = deque(line.split())\n",
    "            token_pos.appendleft(f\"{self.bos_token}{split_token}{self.bos_token}\")\n",
    "            token_pos.append(f\"{self.eos_token}{split_token}{self.eos_token}\")\n",
    "            token_pos = list(token_pos)\n",
    "            for i in range(len(token_pos)-1):\n",
    "                token_i, pos_i = token_pos[i].split(split_token)\n",
    "                token_j, pos_j = token_pos[i+1].split(split_token)\n",
    "                cnt_transition[pos_i][pos_j] += 1\n",
    "                cnt_emission[pos_i][token_i] += 1\n",
    "            else:\n",
    "                token_i, pos_i = token_pos[-1].split(split_token)\n",
    "                cnt_emission[pos_i][token_i] += 1\n",
    "                \n",
    "        return cnt_transition, cnt_emission\n",
    "    \n",
    "    def _get_prob_dict(self, cnt_dict):\n",
    "        prob = defaultdict(lambda: defaultdict(int))\n",
    "        for a, b_cnt in cnt_dict.items():\n",
    "            num_b = sum([cnt for cnt in b_cnt.values()])\n",
    "            for b, cnt in b_cnt.items():\n",
    "                prob[a][b] = cnt / num_b # MLE\n",
    "        return prob\n",
    "        \n",
    "    def train(self, dataset: List[str], split_token=\"_\"):\n",
    "        \"\"\"\n",
    "        Compute transition probability and emission probability with MLE\n",
    "        \"\"\"\n",
    "        cnt_transition, cnt_emission = self._get_count_dict(dataset, split_token)\n",
    "        self.prob_transition = self._get_prob_dict(cnt_transition)\n",
    "        self.prob_emission = self._get_prob_dict(cnt_emission)\n",
    "\n",
    "        self.pos = list(cnt_transition.keys())\n",
    "        \n",
    "    def _viterbi(self, tokens: List[str]):\n",
    "        prob_path = defaultdict(lambda: defaultdict(int)) \n",
    "        backpointer = defaultdict(lambda: defaultdict(str)) \n",
    "        \n",
    "        N = len(self.pos)\n",
    "        T = len(tokens)\n",
    "        \n",
    "        # initialization step\n",
    "        for state in self.pos:\n",
    "            if (state not in self.prob_transition[self.bos_token]) or (tokens[0] not in self.prob_emission[state]):\n",
    "                continue\n",
    "            prob_path[1][state] = self.prob_transition[self.bos_token][state] * self.prob_emission[state][tokens[0]]\n",
    "            backpointer[1][state] = self.bos_token\n",
    "\n",
    "        # recursion step\n",
    "        for t in range(2, T+1):\n",
    "            p_max = 0\n",
    "            current = None\n",
    "            pointer = None\n",
    "            for state in self.pos:\n",
    "                for state_ in self.pos:\n",
    "                    if (state not in self.prob_transition[state_]) or (tokens[t-1] not in self.prob_emission[state]):\n",
    "                        continue\n",
    "                    temp = prob_path[t-1][state_] * self.prob_transition[state_][state] * self.prob_emission[state][tokens[t-1]]\n",
    "                    if temp >= p_max:\n",
    "                        p_max = temp\n",
    "                        current = state\n",
    "                        pointer = state_\n",
    "            else:\n",
    "                prob_path[t][current] = p_max\n",
    "                backpointer[t][current] = pointer\n",
    "        else:\n",
    "            prob_bestpath = 0\n",
    "            pointer = None\n",
    "            for state in self.pos:\n",
    "                if state not in prob_path[T]:\n",
    "                    continue\n",
    "                if prob_path[T][state] > prob_bestpath:\n",
    "                    prob_bestpath = prob_path[T][state]\n",
    "                    pointer = state\n",
    "            bestpath_reversed = [pointer]\n",
    "            for t in range(T, 1, -1):\n",
    "                if backpointer[t][pointer] == \"\":\n",
    "                    bestpath_reversed.append(None)\n",
    "                else:\n",
    "                    bestpath_reversed.append(backpointer[t][pointer])\n",
    "                pointer = backpointer[t][pointer]\n",
    "        \n",
    "        return list(reversed(bestpath_reversed)), prob_bestpath\n",
    "                \n",
    "    def _get_metrics(self, TP, FN, FP):\n",
    "        prec = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * prec * recall / (prec + recall)\n",
    "        return prec, recall, f1\n",
    "    \n",
    "    def _get_scores(self, y_test: List[str], y_pred: List[str]):\n",
    "        assert len(y_test) == len(y_pred)\n",
    "        \n",
    "        scores = defaultdict(lambda: defaultdict(int))\n",
    "        for pos_test, pos_pred in zip(y_test, y_pred):\n",
    "            if pos_test == pos_pred:\n",
    "                scores[pos_pred][\"TP\"] += 1\n",
    "            else:\n",
    "                scores[pos_test][\"FN\"] += 1\n",
    "                scores[pos_pred][\"FP\"] += 1\n",
    "        return scores\n",
    "\n",
    "    def _get_ppl(self, prob, N):\n",
    "        return  (1 / prob) ** (1 / N)\n",
    "        \n",
    "    def evaluate(self, dataset: List[str], split_token=\"_\", return_each_metric=False):\n",
    "        \"\"\"\n",
    "        Return macro precision, macro recall, macro f-score and the average of PPL.\n",
    "        The average of PPL is calculated without the sentences where the path probabilities are assigned as zeros\n",
    "        \"\"\"\n",
    "        \n",
    "        macro_scores = defaultdict(lambda: defaultdict(int))\n",
    "        macro_metrics = defaultdict(lambda: defaultdict(int))\n",
    "        ppl = []\n",
    "        cnt_null = 0\n",
    "        for line in dataset:\n",
    "            tokens = []\n",
    "            y_test = []\n",
    "            for token_pos in line.split():\n",
    "                token, pos = token_pos.split(split_token)\n",
    "                tokens.append(token)\n",
    "                y_test.append(pos)\n",
    "            y_pred, prob = self._viterbi(tokens)\n",
    "            if prob > 0:\n",
    "                ppl.append(self._get_ppl(prob, len(tokens)))\n",
    "            else:\n",
    "                cnt_null += 1\n",
    "            scores = self._get_scores(y_test, y_pred)\n",
    "            for pos, score_cnt in scores.items():\n",
    "                for score, cnt in score_cnt.items():\n",
    "                    macro_scores[pos][score] += cnt\n",
    "        for pos in macro_scores.keys():\n",
    "            if \"TP\" not in macro_scores[pos]:\n",
    "                continue\n",
    "            prec, recall, f1 = self._get_metrics(macro_scores[pos][\"TP\"], macro_scores[pos][\"FN\"], macro_scores[pos][\"FP\"])\n",
    "            macro_metrics[pos][\"Precision\"] = prec\n",
    "            macro_metrics[pos][\"Recall\"] = recall\n",
    "            macro_metrics[pos][\"f-score\"] = f1\n",
    "        ppl_average = sum(ppl) / len(ppl)\n",
    "        \n",
    "        ave_prec = sum([macro_metrics[pos][\"Precision\"] for pos in macro_scores.keys()]) / len(macro_scores.keys())\n",
    "        ave_recall = sum([macro_metrics[pos][\"Recall\"] for pos in macro_scores.keys()]) / len(macro_scores.keys())\n",
    "        ave_f1 = sum([macro_metrics[pos][\"f-score\"] for pos in macro_scores.keys()]) / len(macro_scores.keys())\n",
    "        \n",
    "        print(f\"The path probabilities of {cnt_null} sentences are assigned as zeros.\")\n",
    "        print(f\"Precision: {ave_prec}, Recall: {ave_recall}, f-score: {ave_f1}, Average of PPL: {ppl_average}\")\n",
    "        if return_each_metric:\n",
    "            return ave_prec, ave_recall, ave_f1, ppl_average, macro_metrics\n",
    "        else:\n",
    "            return ave_prec, ave_recall, ave_f1, ppl_average\n",
    "        \n",
    "    def tag(self, text: str, split_token=\"_\"):\n",
    "        \"\"\"\n",
    "        Return a sequence where POS is assigned to each token, with the specified split token.\n",
    "        None is assigned when the model can not predict any POS.\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        result, prob = self._viterbi(tokens)\n",
    "        return \" \".join([f\"{tokens[i]}{split_token}{pos}\" for i, pos in enumerate(result)]), prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../data/wiki-en-train.norm_pos\"\n",
    "test_file_path = \"../data/wiki-en-test.norm_pos\"\n",
    "train_file = load_dataset(train_file_path)\n",
    "test_file = load_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagger = HMMTagger()\n",
    "tagger.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path probabilities of 154 sentences are assigned as zeros.\n",
      "Precision: 0.5632051282051282, Recall: 0.04469060798621794, f-score: 0.08179061260804937, Average of PPL: 405.49789665762137\n"
     ]
    }
   ],
   "source": [
    "results = tagger.evaluate(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('One_CD solution_NN some_DT researchers_NNS have_VBP used_VBN is_VBZ to_TO choose_VB a_DT particular_JJ dictionary_NN ,_, and_CC just_RB use_VB its_PRP$ set_NN of_IN senses_NNS ._.',\n",
       " 1.0115399000049753e-52)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 28\n",
    "split_token = \"_\"\n",
    "text = \" \".join([token_pos.split(\"_\")[0] for token_pos in test_file[idx].split()])\n",
    "tagger.tag(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
