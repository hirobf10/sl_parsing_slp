{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dataset\n",
    "- each line contains: ID, Word, Base, POS, POS2, ?, Head, Type\n",
    "- Tab-separated columns, sentences separated by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../data/mstparser-en-train.dep\"\n",
    "test_file_path = \"../data/mstparser-en-test.dep\"\n",
    "\n",
    "def load_dataset(path, encoding=\"utf-8\"):\n",
    "    with open(path, encoding=encoding) as f:\n",
    "        dataset = f.readlines()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to create features for graph-based dependency parsers?\n",
    "    - I employ the features which appear frequenctly in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        The format of dataset should be CoNLL.\n",
    "        Tab-separated columns, sentences separated by space.\n",
    "        each line contains: ID, Word, Base, POS, POS2, ?, Head, Type.\n",
    "        \"\"\"\n",
    "        self.data = self._format(dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            return [self.data[i] for i in range(*key.indices(len(self.data)))]\n",
    "        elif isinstance(key, int):\n",
    "            if key < 0:\n",
    "                key += len(self.data)\n",
    "            if key < 0 or key >= len(self.data):\n",
    "                raise IndexError\n",
    "            return self.data[key]\n",
    "        else:\n",
    "            raise TypeError\n",
    "    \n",
    "    def _format(self, dataset):\n",
    "        batch = []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i] == \"\\n\":\n",
    "                first = i - int(dataset[i-1].split(\"\\t\")[0])\n",
    "                batch.append([line.split(\"\\n\")[0].split(\"\\t\") for line in dataset[first:i]])\n",
    "        return batch\n",
    "        \n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, dataset, lr=0.01, topn=30, init_func=None):\n",
    "        self.dataset = dataset\n",
    "        self.lr = lr\n",
    "        self.topn = topn\n",
    "        self.init_func = init_func\n",
    "        self.features = None\n",
    "        self.weight = None\n",
    "    \n",
    "    def get_weights(self):\n",
    "        self.features = self._get_features()\n",
    "        self.weights = self._update_weights()\n",
    "        return self.weights\n",
    "    \n",
    "    def _count_features(self):\n",
    "        cnt = defaultdict(lambda: defaultdict(int))\n",
    "        for batch in self.dataset:\n",
    "            for i, data_i in enumerate(batch):\n",
    "                cnt[\"word\"][data_i[1]] += 1\n",
    "                cnt[\"pos\"][data_i[3]] += 1\n",
    "                cnt[\"dep\"][data_i[-1]] += 1\n",
    "                if data_i[-1] == \"ROOT\":\n",
    "                    cnt[\"pos_pos\"][f\"{data_i[3]}_ROOT\"] += 1\n",
    "                    cnt[\"word_pos\"][f\"{data_i[1]}_ROOT\"] += 1\n",
    "                    cnt[\"dep\"][f\"{data_i[-1]}_ROOT\"] += 1\n",
    "                else:\n",
    "                    head_idx = i - int(data_i[0]) + int(data_i[6])    \n",
    "                    head = batch[head_idx]\n",
    "                    cnt[\"word_word\"][f\"{data_i[1]}_{head[1]}\"] += 1\n",
    "                    cnt[\"pos_pos\"][f\"{data_i[3]}_{head[3]}\"] += 1\n",
    "                    cnt[\"word_pos\"][f\"{data_i[1]}_{head[3]}\"] += 1\n",
    "                    cnt[\"pos_word\"][f\"{data_i[3]}_{head[1]}\"] += 1\n",
    "                    cnt[\"dep_word\"][f\"{data_i[-1]}_{head[1]}\"] += 1\n",
    "                    cnt[\"dep_pos\"][f\"{data_i[-1]}_{head[3]}\"] += 1\n",
    "        return cnt\n",
    "    \n",
    "    def _get_features(self):\n",
    "        \"\"\"\n",
    "        This function extract features for graph-based parser.\n",
    "        Features include word form, pos, combination of pos and word form.\n",
    "        \"\"\"\n",
    "        cnt = self._count_features()\n",
    "        features = []\n",
    "        for feat_type in cnt.keys():\n",
    "            if feat_type == \"pos_pos\":\n",
    "                for feat, v in cnt[feat_type].items():\n",
    "                    if v >= 5:\n",
    "                        features.append(feat)\n",
    "            else:\n",
    "                for feat, v in sorted(cnt[feat_type].items(), key=lambda i: i[1], reverse=True)[:self.topn]:\n",
    "                    features.append(feat)\n",
    "        return features\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        weights = defaultdict(int)\n",
    "        for feat in self.features:\n",
    "            if self.init_func:\n",
    "                weights[feat] = self.init_func()\n",
    "            else:\n",
    "                weights[feat] = random.random()\n",
    "        return weights\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        w = self._init_weights()\n",
    "        for batch in self.dataset:\n",
    "            score = defaultdict(int)\n",
    "            for i, data_i in enumerate(batch):\n",
    "                for j, data_j in enumerate(batch):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    score[j] += w[data_i[1]]\n",
    "                    score[j] += w[data_i[3]]\n",
    "                    score[j] += w[data_i[-1]]\n",
    "                    score[j] += w[f\"{data_i[1]}_{data_j[1]}\"]\n",
    "                    score[j] += w[f\"{data_i[1]}_{data_j[3]}\"]\n",
    "                    score[j] += w[f\"{data_i[3]}_{data_j[1]}\"]\n",
    "                    score[j] += w[f\"{data_i[3]}_{data_j[3]}\"]\n",
    "                    score[j] += w[f\"{data_i[-1]}_{data_j[1]}\"]\n",
    "                    score[j] += w[f\"{data_i[-1]}_{data_j[3]}\"]\n",
    "                score[\"ROOT\"] += w[f\"{data_i[1]}_ROOT\"]\n",
    "                score[\"ROOT\"] += w[f\"{data_i[3]}_ROOT\"]\n",
    "                score[\"ROOT\"] += w[f\"{data_i[-1]}_ROOT\"]\n",
    "                if len(score) == 0:\n",
    "                    continue\n",
    "                head_pred = sorted(score.items(), key=lambda i: i[1], reverse=True)[0][0]\n",
    "                head_true = data_i[6]\n",
    "                if head_pred == \"ROOT\":\n",
    "                    if data_i[-1] != \"ROOT\":\n",
    "                        w[data_i[1]] -= self.lr\n",
    "                        w[data_i[3]] -= self.lr\n",
    "                        w[f\"{data_i[1]}_ROOT\"] -= self.lr\n",
    "                        w[f\"{data_i[3]}_ROOT\"] -= self.lr\n",
    "                        w[f\"{data_i[-1]}_ROOT\"] -= self.lr\n",
    "                elif head_pred != head_true:\n",
    "                    data_j = batch[head_pred]\n",
    "                    w[data_i[1]] -= self.lr\n",
    "                    w[data_i[3]] -= self.lr\n",
    "                    w[data_i[-1]] -= self.lr\n",
    "                    w[f\"{data_i[1]}_{data_j[1]}\"] -= self.lr\n",
    "                    w[f\"{data_i[1]}_{data_j[3]}\"] -= self.lr\n",
    "                    w[f\"{data_i[3]}_{data_j[1]}\"] -= self.lr\n",
    "                    w[f\"{data_i[3]}_{data_j[3]}\"] -= self.lr\n",
    "                    w[f\"{data_i[-1]}_{data_j[1]}\"] -= self.lr\n",
    "                    w[f\"{data_i[-1]}_{data_j[3]}\"] -= self.lr\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(load_dataset(train_file_path))\n",
    "test_dataset = Dataset(load_dataset(test_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSTParser:\n",
    "    def __init__(self, extractor):\n",
    "        self.w = extractor.get_weights()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return self.parse(batch)\n",
    "    \n",
    "    def parse(self, batch):\n",
    "        scores = self._score(batch)\n",
    "        best_edges = self._get_best_edges(scores)\n",
    "        scores = self._subtract(scores, best_edges)\n",
    "        best_edges = self._get_best_edges(scores)\n",
    "        status, cycles = self._involveCycle([edge for edge, score in best_edges])\n",
    "        if status:\n",
    "            best_edges = self._remove_cycles(scores, cycles, best_edges)\n",
    "        return [edge for edge, score in best_edges[1:]]\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        UAS_SUM = 0\n",
    "        for batch in dataset:\n",
    "            # Parse a sentence\n",
    "            scores = self._score(batch)\n",
    "            best_edges = self._get_best_edges(scores)\n",
    "            scores = self._subtract(scores, best_edges)\n",
    "            best_edges = self._get_best_edges(scores)\n",
    "            status, cycles = self._involveCycle([edge for edge, score in best_edges])\n",
    "            if status:\n",
    "                best_edges = self._remove_cycles(scores, cycles, best_edges)\n",
    "            y_test = self._extract_test(batch)\n",
    "            y_pred = [edge for edge, score in best_edges[1:]]\n",
    "            UAS_SUM += self._get_UAS(y_test, y_pred)\n",
    "        return UAS_SUM / len(dataset)\n",
    "            \n",
    "    \n",
    "    def _score(self, batch):\n",
    "        N = len(batch)\n",
    "        scores  = np.ones([N+1, N+1]) * -1e3\n",
    "        for i in range(1, N+1):\n",
    "            for j in range(N+1):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                score = 0\n",
    "                score += self.w[batch[i-1][1]] #word\n",
    "                score += self.w[batch[i-1][3]] # pos\n",
    "                score += self.w[batch[i-1][-1]]\n",
    "                if j == 0:\n",
    "                    score += self.w[f\"{batch[i-1][1]}_ROOT\"] + 1 \n",
    "                    score += self.w[f\"{batch[i-1][3]}_ROOT\"] + 1\n",
    "                    score += self.w[f\"{batch[i-1][-1]}_ROOT\"] + 1\n",
    "                else:\n",
    "                    score += self.w[f\"{batch[i-1][1]}_{batch[j-1][1]}\"]\n",
    "                    score += self.w[f\"{batch[i-1][1]}_{batch[j-1][3]}\"]\n",
    "                    score += self.w[f\"{batch[i-1][3]}_{batch[j-1][1]}\"]\n",
    "                    score += self.w[f\"{batch[i-1][3]}_{batch[j-1][3]}\"]\n",
    "                    score += self.w[f\"{batch[i-1][-1]}_{batch[j-1][1]}\"]\n",
    "                    score += self.w[f\"{batch[i-1][-1]}_{batch[j-1][3]}\"]\n",
    "                scores[i, j] = score\n",
    "        return scores\n",
    "    \n",
    "    def _get_best_edges(self, scores):\n",
    "#         ignore 0-th row because it would contain scores between ROOT as dependent and words as head\n",
    "        return [(np.argmax(scores[i, :]),\n",
    "                 np.max(scores[i, :]))\n",
    "                if i != 0 else (-1, -1e3)\n",
    "                for i in range(scores.shape[0])\n",
    "                ]\n",
    "    \n",
    "#         best_edges = []\n",
    "#         root_child_idx = np.argmax(scores[:, 0])\n",
    "#         for i in range(scores.shape[0]):\n",
    "#             if i == root_child_idx:\n",
    "#                 best_edges.append((0, scores[root_child_idx, 0]))\n",
    "#             elif i == 0:\n",
    "#                 best_edges.append((-1, -1e3))\n",
    "#             else:\n",
    "#                 head_idx = np.argmax(scores[i, 1:])\n",
    "#                 best_edges.append((head_idx, scores[i, head_idx]))\n",
    "#         return best_edges\n",
    "        \n",
    "    def _subtract(self, scores, best_edges):\n",
    "        N = scores.shape[0]\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i == 0 or i == j:\n",
    "                    continue\n",
    "                scores[i, j] -= best_edges[i][1]\n",
    "        return scores\n",
    "        \n",
    "    def _involveCycle(self, edges):\n",
    "        memory = []\n",
    "        for dep, head in enumerate(edges):\n",
    "            dep_ = edges[head]\n",
    "            if dep == dep_ and (sorted([dep, head]) not in memory):\n",
    "                memory.append(sorted([dep, head]))\n",
    "        if memory:\n",
    "            return (True, memory)\n",
    "        else:\n",
    "            return (False, [])\n",
    "    \n",
    "    def _remove_cycles(self, scores, cycles, best_edges):\n",
    "        N = scores.shape[0]\n",
    "        for cycle in cycles:\n",
    "            scores_ = scores.copy()\n",
    "            scores_[cycle[0], cycle[1]] = -1e3\n",
    "            scores_[cycle[1], cycle[0]] = -1e3\n",
    "            node, j = divmod(np.argmax(scores_[cycle, :]), N)\n",
    "            if node == 0:\n",
    "                c_head = cycle[0]\n",
    "            else:\n",
    "                c_head = cycle[1]\n",
    "            best_edges[c_head] = (j, scores[c_head, j])\n",
    "        return best_edges\n",
    "    \n",
    "    def _extract_test(self, batch):\n",
    "        return [int(data[6]) for data in batch]\n",
    "    \n",
    "    def _get_UAS(self, y_test, y_pred):\n",
    "        assert len(y_test) == len(y_pred)\n",
    "        match_num = 0\n",
    "        for test, pred in zip(y_test, y_pred):\n",
    "            if test == pred:\n",
    "                match_num += 1\n",
    "        return match_num / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26872459645161556"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = FeatureExtractor(train_dataset, topn=100, lr=0.1, init_func=None)\n",
    "parser = MSTParser(extractor)\n",
    "parser.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'robert', 'robert', 'NNP', 'NNP', '_', '2', 'DEP'],\n",
       " ['2', 'erwin', 'erwin', 'NNP', 'NNP', '_', '4', 'NP'],\n",
       " ['3', ',', ',', ',', ',', '_', '4', 'DEP'],\n",
       " ['4', 'president', 'president', 'NN', 'NN', '_', '8', 'NP-SBJ'],\n",
       " ['5', 'of', 'of', 'IN', 'IN', '_', '4', 'PP'],\n",
       " ['6', 'biosource', 'biosource', 'NNP', 'NNP', '_', '5', 'NP'],\n",
       " ['7', ',', ',', ',', ',', '_', '4', 'DEP'],\n",
       " ['8', 'called', 'called', 'VBD', 'VBD', '_', '0', 'ROOT'],\n",
       " ['9', 'plant', 'plant', 'NNP', 'NNP', '_', '11', 'DEP'],\n",
       " ['10', 'genetic', 'genetic', 'NNP', 'NNP', '_', '11', 'DEP'],\n",
       " ['11', \"'s\", \"'s\", 'POS', 'POS', '_', '12', 'NP'],\n",
       " ['12', 'approach', 'approach', 'NN', 'NN', '_', '23', 'NP-SBJ'],\n",
       " ['13', '``', '``', '``', '``', '_', '23', 'DEP'],\n",
       " ['14', 'interesting', 'interesting', 'JJ', 'JJ', '_', '23', 'DEP'],\n",
       " ['15', \"''\", \"''\", \"''\", \"''\", '_', '23', 'DEP'],\n",
       " ['16', 'and', 'and', 'CC', 'CC', '_', '23', 'DEP'],\n",
       " ['17', '``', '``', '``', '``', '_', '23', 'DEP'],\n",
       " ['18', 'novel', 'novel', 'JJ', 'JJ', '_', '23', 'DEP'],\n",
       " ['19', ',', ',', ',', ',', '_', '23', 'DEP'],\n",
       " ['20', \"''\", \"''\", \"''\", \"''\", '_', '23', 'DEP'],\n",
       " ['21', 'and', 'and', 'CC', 'CC', '_', '23', 'DEP'],\n",
       " ['22', '``', '``', '``', '``', '_', '23', 'DEP'],\n",
       " ['23', 'complementary', 'complementary', 'JJ', 'JJ', '_', '8', 'S'],\n",
       " ['24', 'rather', 'rather', 'RB', 'RB', '_', '25', 'DEP'],\n",
       " ['25', 'than', 'than', 'IN', 'IN', '_', '23', 'PP'],\n",
       " ['26', 'competitive', 'competitive', 'JJ', 'JJ', '_', '25', 'ADJP'],\n",
       " ['27', '.', '.', '.', '.', '_', '8', 'DEP'],\n",
       " ['28', \"''\", \"''\", \"''\", \"''\", '_', '8', 'DEP']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
